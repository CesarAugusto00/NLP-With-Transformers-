# -*- coding: utf-8 -*-
"""GPT-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M1bqSKS8_BfKftN1di1toHHOMpGnYLEV
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd

device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "gpt2-xl"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

"""# How GPT-2 Predicts the Next Word: Example Show Case"""

#This code is to feed the model the previous input to predict the next token output
input_txt = "My pants are"

#Tokenize the input. Note make sure not to add an space at end
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
iterations = []

#Number of words added to the sentence
n_steps = 10
#Show the first 5 choices from the model
choices_per_step = 5

with torch.no_grad():
    for _ in range(n_steps):
        iteration = dict()
        iteration["Input"] = tokenizer.decode(input_ids[0])
        output = model(input_ids=input_ids)
        # Select logits of the first batch and the last token and apply softmax
        next_token_logits = output.logits[0, -1, :]
        next_token_probs = torch.softmax(next_token_logits, dim=-1)
        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)
        # Store tokens with highest probabilities
        for choice_idx in range(choices_per_step):
            token_id = sorted_ids[choice_idx]
            token_prob = next_token_probs[token_id].cpu().numpy()
            token_choice = (
                f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)"
            )
            iteration[f"Choice {choice_idx+1}"] = token_choice
        # Append predicted next token to input
        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)
        iterations.append(iteration)

pd.DataFrame(iterations)



"""##Generating the prediction using generate()"""

input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
#Note that in amx tokens the number of predictions is specified
output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)
print(tokenizer.decode(output[0]))

#Generating autocompletition with a length of 128

#Note we get as output a loof of the last two sentences. This is common see in a greedy selection
max_length = 128
input_txt = """It has recently been confirm the possible meeting with  \
outside life forms with the principal representants of the principal countries  \
The meeting will take place on Mexico with the president already preparing a contingency plan \
Nothing is known yet about the outside life form.\n\n
"""
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output_greedy = model.generate(input_ids, max_length=max_length,
                               do_sample=False)
print(tokenizer.decode(output_greedy[0]))

"""# Comparing the log probability of Greedy Decoding and Beam Search Decoding  """

import torch.nn.functional as F

def log_probs_from_logits(logits, labels):
    logp = F.log_softmax(logits, dim=-1)
    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)
    return logp_label

def sequence_logprob(model, labels, input_len=0):
    with torch.no_grad():
        output = model(labels)
        log_probs = log_probs_from_logits(
            output.logits[:, :-1, :], labels[:, 1:])
        seq_log_prob = torch.sum(log_probs[:, input_len:])
    return seq_log_prob.cpu().numpy()

#Greedy decoding
logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))
print(tokenizer.decode(output_greedy[0]))
print(f"\nlog-prob: {logp:.2f}")

#Beam decoding
output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)
logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))
print(f"\nlog-prob: {logp:.2f}")

#Setting no_repeat_ngram_size
output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,
                             do_sample=False, no_repeat_ngram_size=2)
logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))
print(f"\nlog-prob: {logp:.2f}")

"""# Using Sampling Methods"""

#Setting Temperature to T = .5

output_temp = model.generate(input_ids, max_length=max_length, do_sample=True,
                             temperature=0.5, top_k=0)
print(tokenizer.decode(output_temp[0]))

"""## Using Top-k and Nucleus Sampling"""

#Using top-k
output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,
                             top_k=50)
print(tokenizer.decode(output_topk[0]))

#Using top-p sampling methods
output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,
                             top_p=0.90)
print(tokenizer.decode(output_topp[0]))